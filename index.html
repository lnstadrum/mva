<html>
<head>
  <link rel="stylesheet" href="revealjs/dist/reveal.css">
  <link rel="stylesheet" href="revealjs/dist/theme/white.css">
  <script src="revealjs/dist/reveal.js"></script>
  <script src="revealjs/plugin/markdown/markdown.js"></script>

  <style>
  .reveal {
    --r-heading-text-transform: '';
  }
  .smaller {
    font-size: 0.8em;
  }
  .code {
    font-family: monospace;
  }

  li {
    line-height: 1.1;
    margin-top: 0.5em;
  }

  /* nested list indent */
  li > ul {
    padding-left: 0;
  }

  footer {
    text-align: right;
    font-size: 0.5em;
    margin-top: 3em;
  }
  </style>
</head>
<body>
<div class="reveal">
<div class="slides">


<!-- Introduction -->
<section>
  <section>
    <h1>Imaging and ML software from a low-level programming perspective</h1>
    <h5 style="color: red;">{WORK IN PROGRESS}</h5>
  </section>

  <section>
    <h2>Imaging Software in Production: Basics</h2>
    <img src="diagrams/Intro.drawio.svg" class="r-stretch"/>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Imaging Software in Production
    - Designing production-ready imaging algorithms and NN architectures is a challenge.
    - **Speed**, **memory** and **energy footprints** are constrained.
    - Algorithmic optimization incurs performance losses.
    - Knowing your hardware at a low level helps.
    </textarea>
  </section>

  <section>
    <h2>ML in production: market</h2>
    <img src="diagrams/MLOps.drawio.svg" class="r-stretch"/>
    <footer>
      <a href="https://techcrunch.com/2021/11/01/octoml-raises-85m-for-it-for-its-machine-learning-acceleration-platform/">[source]</a>
      <a href="https://www.forbes.com/sites/robtoews/2022/12/20/10-ai-predictions-for-2023/">[source]</a>
      <a href="https://3dprintingindustry.com/news/nano-dimension-acquires-ai-company-deepcube-for-70m-189002/">[source]</a>
      <a href="https://techcrunch.com/2022/12/06/neureality-ai-accelerator-chips-startup-raises-35m/">[source]</a>
      <a href="https://www.prweb.com/releases/neural_magic_announces_30_million_series_a_funding_led_by_nea/prweb18240687.htm">[source]</a>
      <a href="https://www.forbes.com/sites/patrickmoorhead/2022/08/11/memryx-is-a-new-ai-company-we-actually-need/">[source]</a>
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## From Computer Vision to Image Processing
    - Semantic gap
    - Vision models usually map small images to low-dimensional spaces.
      - ILSVRC classic models take a 224*224 image to output a 1000-long vector.
    - Image restoration algorithms typically take and produce images of the same high resolution.
      - Standard smartphone camera photo resolution is 4000*3000.
    </textarea>
  </section>
</section>


<!-- Outline -->
<section style="font-size: 0.8em">
  <h1>Outline</h1>
  <ol>
    <li style="color: lightgray">Introduction</li>

    <li>Programmable Hardware Overview
      <ul class="smaller">
        <li>CPU</li>
        <li>GPU
          <!-- <ul>
            <li>History: from fixed graphic pipeline to modern programmable GPU</li>
            <li>Memory model overview</li>
            <li>SIMT vs SIMD</li>
            <li>CUDA, OpenCL, GLSL</li>
          </ul> -->
        </li>
        <li>DSP</li>
      </ul>
    </li>

    <li>Deploying to production
      <ul class="smaller">
        <li>Algorithm characterization</li>
        <li>Common ML model optimization techniques
          <ul>
            <li>Quantization and QAT</li>
            <li>Pruning</li>
          </ul>
        </li>
      </ul>
    </li>

    <li>Conclusion</li>
  </ol>
</section>


<!-- Hardware overview  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
  </section>

  <section>
    <h2>Vanilla server/desktop system</h2>

    <img src="diagrams/PC.svg" class="r-stretch"/>

    <footer>
      <a href="https://en.wikipedia.org/wiki/File:Desktop_computer_bus_bandwidths.svg">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Mobile SoC (System on Chip)</h2>
    <h6>Example of Qualcomm&copy; Snapdragon&trade; 8+ Gen 1 (2022)</h6>

    <img src="diagrams/SoC.drawio.svg" class="r-stretch"/>

    <footer>
      <a href="https://www.forbes.com/sites/marcochiappetta/2021/11/30/snapdragon-8-gen-1-the-qualcomm-mobile-platform-that-will-power-next-gen-android-flagship-phones/">[source]</a>
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Mobile vs Desktop: Initial Observations
    - No unified memory in the desktop world.
      - Think ahead about CPU&leftrightarrow;GPU data transfer.
      - PCIe is the weak link (~10 times slower than the GPU VRAM bandwidth).
    - Modern x86 desktop CPUs also include integrated graphics (iGPU) without dedicated memory.
    - Digital Signal Processors (DSP) are very common in mobile SoCs.
      - *Edge computing*: processing data right on the device it comes from.
      - NPU is a marketing name/abstraction level on top of the DSP.
    </textarea>
  </section>
</section>


<!-- Hardware overview: CPU  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
    <strong>C</strong>entral <strong>P</strong>rocessing <strong>U</strong>nits
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Basics
    - CPU is a general-purpose programmable device running programs built of *instructions.*
      - A program is usually generated *compilation toolchains* from a higher-level source code.
    - *Instruction set* is the language understandable by a specific CPU architecture.
    - Common instruction types:
      - setting, loading and storing values of *registers*,
      - arithmetic operations with the registers content,
      - control flow (branches and jumps).
    </textarea>
  </section>

  <section>
    <h2>Common Architectures</h2>
    <table class="smaller">
      <thead>
        <tr>
          <td style="color: gray">Family</td>
          <td><strong>x86(_64)</strong></td>
          <td><strong>ARM</strong></td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="color: gray">Instruction set<br/>classification</td>
          <td>Complex Instruction Set Computer</br>(CISC)</td>
          <td>Reduced Instruction Set Computer</br>(RISC)</td>
        </tr>
        <tr>
          <td style="color: gray">Usage</td>
          <td>Mainly desktop/data center.<br/>Weak mobile market penetration</td>
          <td>Mainly embedded.<br/>About to change: <a href="https://www.nvidia.com/en-us/data-center/grace-cpu-superchip/">Nvidia Grace</a></td>
        </tr>
        <tr>
          <td style="color: gray">Key brand names</td>
          <td>Intel, AMD</td>
          <td>ARM<br/>Apple, Samsung, Qualcomm, ...</td>
        </tr>
        <tr>
          <td style="color: gray">Vector extensions (SIMD)</td>
          <td>MMX, SSE, FMA, AVX, ...</td>
          <td>NEON</td>
        </tr>
        <tr>
          <td style="color: gray">Word size</td>
          <td colspan="2" style="text-align: center">32 or 64 bits</td>
        </tr>
        <tr>
          <td style="color: gray">Simultaneous<br/>multithreading<br/>aka "Hyper-Threading"</td>
          <td>Yes:<br/>commonly 2 logical per 1 physical</td>
          <td>No, till recently: <a href="https://www.arm.com/products/silicon-ip-cpu/cortex-a/cortex-a65ae">Cortex A65</a></td>
        </tr>
      </tbody>
    </table>

    <p class="smaller">Other architectures exist. To keep an eye on: RISC-V.</p>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Characterizing Execution Speed
     - *Clock cycle*: atomic value of elapsed time.
     - *Latency* of a given instruction: number of cycles the *CPU pipeline* takes to execute the instruction.
     - Modern CPUs are complex machines and may execute multiple instructions of the same program at a time.
       - *Out-of-Order Execution*: if the next instruction in a program does not depend on the result of the current instruction, it can be issued to the pipeline prior to finishing the current instruction.
       - Instruction latencies are context-dependent but measurable.
    </textarea>
  </section>

  <section>
    <h2>Arithmetic Instructions Latency</h2>
    <ul>
      Operands are registers.
      <li>Integer addition/subtraction: as fast as 1 cycle.</li>
      <li>Integer multiplication and floating-point math: a few cycles.</li>
      <li>Division: more.
        <ul class="smaller">
          <li>Example: 12 cycles per 32 bit integer division on Intel Skylake.</li>
        </ul>
      </li>
      <li>Multiplication-accumulation is typically a single 3-op instruction and is done faster than a multiplication and an addition separately.</li>
      <li>Complex floating-point math instructions (e.g. <span class="code">sqrt</span>) may have content-dependent latency.</li>
    </ul>

    <footer>
      Unofficial but highly acknowledged sources for <a href="https://www.agner.org/optimize/instruction_tables.pdf">x86</a> and <a href="https://hardwarebug.org/2014/05/15/cortex-a7-instruction-cycle-timings/">ARM</a>.
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Memory Instructions Latency
     - To speak about loads/stores latency, we need to introduce the *memory model* first.
     - *Locality of reference:* a program usually accesses the same set of addresses over a short period of time.
       - Does not always hold for imaging/ML-related compute code.
     - *Memory hierarchy*:
       1. **Register file**: small amount of immediately available memory directly usable by the compute units (~1 Kbyte).
       2. **Cache**: fast on-chip memory, usually of multiple levels (L1, L2, ...), from a few Kbytes to Mbytes each.
       3. **Main memory** (RAM): slow external memory, from Gbytes to Tbytes.
     - From imaging algorithms perspective, caches are ridiculously small.
     - *Cache miss*: event occurring when the requested memory address is not found in a given cache.
     - *Stalled thread* is the one getting a cache miss.
    </textarea>
  </section>

  <section>
    <h2>Memory Access Speed: Some Figures</h2>
    <div style="display: flex;">
      <div>
        <h5>Intel Core i7-9xxx:</h5>
        <ul>
          <li>L1 cache: 64 Kbytes per physical core, latency: <strong>4&nbsp;cycles</strong>
            <ul class="smaller">
              <li>32 Kbytes instruction cache + 32 Kbytes data cache</li>
            </ul>
          </li>
          <li>L2 cache: 256 Kbytes per physical core, latency: <strong>11&nbsp;cycles</strong></li>
          <li>L3 cache: 8 Mbytes shared across cores, latency: <strong>39&nbsp;cycles</strong></li>
          <li>RAM latency: <strong>107&nbsp;cycles</strong>.</li>
        </ul>
      </div>
      <div>
        <h5>A synthetic example:</h5>
        <img src="images/memory_model.png" style="margin: 0em">
      </div>
    </div>
    <footer>
      <a href="https://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf">[source]</a>
      <a href="https://cs.brown.edu/courses/csci1310/2020/assign/labs/lab4.html">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Cache Misses: What To Do?</h2>
    <ul style="list-style-type: none">
      <li><em>Cache line</em>: atomic amount of data readable/writable to cache and main memory.
        <ul>
          <li>Usually 64 bytes.</li>
        </ul>
      </li>
      <li><strong>Caching-friendly practices:</strong>
        <ol>
          <li>Coalesced memory access
            <ul class="smaller">
              <li>If you read from/write to a particular address in memory, make sure the next read/write will hit an adjacent address in as many cases as possible.</li>
            </ul>
          </li>
          <li>Prefetching
            <ul class="smaller">
              <li>x86 CPUs detect linear forward and backward traversal patterns of a cache line <span class="code">n</span> and automatically prefetch line <span class="code">n+1</span>/<span class="code">n-1</span>.</li>
              <li>ARM instruction set contains a <em>prefetch instruction</em> allowing to give a hint to the memory management hardware to preload a cache line containing a specific address ahead of processing.</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </section>

  <section>
    <h2>Instructions Latency: Recap</h2>
    <ul>
      <li>Basic compute instructions are bounded in latency by a few cycles.</li>
      <li>Memory instructions have a largely variable latency depending where the requested data is in the memory hierarchy.
        <ul>
          <li>Regardless of how well you optimize your compute code, if you do not take care of caching, your will experience a dramatic performance drop.</li>
          <li>Coalesced predicable linear access is the most caching-friendly way to hit the data.</li>
        </ul>
      </li>
    </ul>
    <footer>
      <q>It takes so long to get there [to main memory], your program will not run at any reasonable speed if you go to main memory.</q>
      <a href="https://youtu.be/WDIkqP4JbkE">[Scott Meyers]</a>
    </footer>
  </section>
</section>


<!-- Hardware overview: GPU  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
    <strong>G</strong><span>aming</span> <strong>P</strong>rocessing <strong>U</strong>nits
  </section>

  <section data-markdown>
    <textarea data-template>
    ## GPU: Brief History
     - In early times (~2000 AD), GPUs were <em>rasterization engines</em>.
       - They sample images pixel by pixel from a vector description of a renderable content, e.g. set of 3D vertices, their attributes (such as texture coordinates), light source positions and textures).
       - Suitable for real-time visualization and... gaming.
       - They featured *fixed graphics pipeline* (no programmability).
     - *Pixel shaders*: small programs executed on per-pixel basis to compute its resulting color value (~2001).
       - Beginning of per-pixel thread granularity, hardly achievable on CPUs.
       - Nvidia GeForce 3 (NV20) and AMD Radeon R100
     - *Microsoft DirectX* and *OpenGL* standards have driven the GPU evolution towards programmability.
       - *GPGPU*: General-Purpose computing on Graphics Processing Units
    </textarea>
  </section>

  <section>
    <h2>Programmable Shading Applied to Lighting</h2>
    <img class="r-stretch" src="images/unreal_shading_example.jpg"/>
    <footer style="margin: 0">
      <a href="https://www.nvidia.com/content/gtc-2010/pdfs/2275_gtc2010.pdf">[source]</a>
    </footer>
  </section>

  <section>
    <h2>GPU Compute Evolution: Back Then</h2>
    <img src="images/nvidia_gflops_evolution.png" class="r-stretch"/>
    <p class="smaller">*AlexNet was trained on two GTX580 <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">[Krizhevsky et al.]</a></p>
    
    <footer style="margin: 0">
      <a href="https://www.researchgate.net/publication/272434636_Accelerating_the_Detection_of_Spectral_Bands_by_ANN-ED_on_a_GPU#pf2">[source]</a>
    </footer>
  </section>

  <section>
    <h2>GPU Compute Evolution: These Days</h2>
    <ul>
      <li><em>Quadro RTX 6000</em> (Turing architecture, 2018):
        <ul style="list-style-type: none">
          <li><strong>16.3 TFLOPs</strong> single precision FP</li>
          <li><strong>32.6 TFLOPs</strong> half precision FP</li>
          <li><strong>130.5 TFLOPs</strong> half precision FP with TensorCores</li>
        </ul>
      </li>
      <li><em>Tesla A100</em> (Ampere architecture, 2020):
        <ul style="list-style-type: none">
          <li><strong>19.5 TFLOPs</strong> single precision FP</li>
          <li><strong>156 TFLOPs</strong> single precision FP with TensorCores</li>
          <li><strong>312 TFLOPs</strong> half precision FP with TensorCores</li>
        </ul>
      </li>
      <li><em>Tesla H100</em> (Hopper architecture, 2022):
        <ul style="list-style-type: none">
          <li><strong>48 TFLOPs</strong> single precision FP</li>
          <li><strong>400 TFLOPs</strong> single precision FP with TensorCores</li>
          <li><strong>800 TFLOPs</strong> half precision FP with TensorCores</li>
          <li><strong>1600 TFLOPs</strong> 8-bit FP with TensorCores</li>
        </ul>
      </li>
    </ul>
    <footer style="margin: 0">
      <a href="https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/">[source]</a>
      <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">[source]</a>
      <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">[source]</a>
    </footer>
  </section>

  <section>
    <h2>GPU Programming</h2>
    <ul>
      <li>
        <strong>CUDA</strong> (Computing Using Digital Accelerators)
        <ul>
          <li>Nvidia GPU-specific API</li>
          <li>C/C++-like language and a compilation toolchain</li>
          <li>The toolchain is bundled with precompiled libraries implementing common algorithms (cuBLAS, cuDNN, cuFFT, etc.)</li>
          <li>Main compute backend for modern ML: PyTorch and TensorFlow rely heavily on cuDNN/cuBLAS</li>
        </ul>
      </li>
      <li>
        <strong>ROCm</strong> (Radeon Open Compute)
        <ul>
          <li>AMD GPU-specific</li>
          <li>Similar to CUDA, but open-source</li>
        </ul>
      </li>
      <li>
        <strong>OpenCL</strong>
        <ul>
          <li>Parallel programming framework to build programs from a C-like language</li>
          <li>Usable with a wide hardware spectrum, including GPUs, CPUs, FPGAs and other kinds of accelerators</li>
          <li>Hardware vendor-agnostic</li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>GPU Programming: GLSL</h2>
    <ul>
      <li>
        <strong>GLSL</strong> (OpenGL Shading Language)
        <ul>
          <li>GPU vendor-agnostic, usable with literally any modern GPU</li>
          <li>
            Originally computer graphics-specific, now enables general purpose parallel programming (<em>compute shaders</em>)
            <ul>
              <li>Compute shaders are not available everywhere</li>
            </ul>
          </li>
          <li>Requires a host environment (OpenGL, Vulkan, WebGL)</li>
          <li>Typically compiled at run time by the GPU driver to the GPU-specific machine code</li>
        </ul>
      </li>
    </ul>
  </section>

</section>

</div>
</div>

<script>
  Reveal.initialize({
    width: 1900,
    height: 1080,
    hash: true,
    plugins: [ RevealMarkdown ]
  });
</script>
</body>
</html>

<html>
<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css"  integrity="sha512-USp+nLNMZ1hR0Ll/LpYDxIq47Ypcm3KfjIleOnyFrB1N5KfHLXjfPQD1wQlhv7kVHRRgPvNVtendDS72LyHviA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/white.min.css" integrity="sha512-RoEy1xOWgzryoplQ1g0eNHizP2BRqEq8eLGpRyI6OvM2FvmxpUyiPotvW4rp/I67VN6eFq4DDXQNcFKbNZ5Rrg==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/monokai.min.css" integrity="sha512-z8wQkuDRFwCBfoj7KOiu1MECaRVoXx6rZQWL21x0BsVVH7JkqCp1Otf39qve6CrCycOOL5o9vgfII5Smds23rg==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.js" integrity="sha512-uQGK5PLqAxTaCXSpiQXLYo93HzIRCBUek4jab5gDky9RN621pYpOh1oSxfmG301wW6OU4s8Bk0JDfi77JMZRuQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <style>
  .reveal {
    --r-heading-text-transform: '';
  }
  .smaller {
    font-size: 0.9em;
  }
  .code {
    font-family: monospace;
  }

  li {
    line-height: 1.1;
    margin: 0.3em 0;
  }

  /* nested list vertical gutter */
  li:last-child {
    margin-bottom: 0.5em;
  }

  /* nested list indent */
  li > ul {
    font-size: 0.9em;
    padding-left: 0;
  }

  footer {
    text-align: right;
    font-size: 0.5em;
    margin-top: 3em;
  }

  .flex {
    display: flex;
    align-items: center;
  }
  .flex-col { flex: 1 1 100%; }
  .flex-col-25 { flex: 1 1 25%; }
  .flex-col-50 { flex: 1 1 50%; }
  .flex-col-75 { flex: 1 1 75%; }

  .no-bullets {
    list-style-type: none !important;
  }
  .arrow-bullets {
    list-style-type: "→  " !important;
  }
  .check-bullets {
    list-style-type: "✓ " !important;
  }
  .cross-bullets {
    list-style-type: "✗ " !important;
  }

  .highlight, blockquote {
    border-left: 20px solid #ddd; border-radius: 20px; padding-left: 20px; margin-left: -40px;
  }
  </style>
</head>
<body>
<div class="reveal">
<div class="slides">


<!-- Introduction -->
<section>
  <section>
    <h1>Imaging and ML software from a low-level programming perspective</h1>
    <h4>Maxim Karpushin</h4>
    <h4>Wolf Hauser</h4>
    <footer style="height: 4em">
      <img src="images/dxo_logo.png">
    </footer>
  </section>

  <section>
    <h2>Imaging Software in Production: Context</h2>
    <img src="diagrams/Intro.drawio.svg" class="r-stretch"/>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Imaging Software in Production
    - Designing production-ready imaging algorithms and NN architectures is a challenge.
       - *Software 2.O*: ML models are not easier to put into production just because they do not lead to segfaults.
    - **Speed**, **memory** and **energy footprints** are constrained.
    - Algorithmic optimization may involve performance/quality tradeoffs.
    - Knowing your hardware at a low level helps.
    </textarea>
  </section>

  <section>
    <h2>ML in Production: Market Dynamics</h2>
    <img src="diagrams/MLOps.drawio.svg" class="r-stretch"/>
    <footer>
      <a href="https://techcrunch.com/2021/11/01/octoml-raises-85m-for-it-for-its-machine-learning-acceleration-platform/">[source]</a>
      <a href="https://www.forbes.com/sites/robtoews/2022/12/20/10-ai-predictions-for-2023/">[source]</a>
      <a href="https://3dprintingindustry.com/news/nano-dimension-acquires-ai-company-deepcube-for-70m-189002/">[source]</a>
      <a href="https://techcrunch.com/2022/12/06/neureality-ai-accelerator-chips-startup-raises-35m/">[source]</a>
      <a href="https://www.prweb.com/releases/neural_magic_announces_30_million_series_a_funding_led_by_nea/prweb18240687.htm">[source]</a>
      <a href="https://www.forbes.com/sites/patrickmoorhead/2022/08/11/memryx-is-a-new-ai-company-we-actually-need/">[source]</a>
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Compute-bound vs I/O-bound Algorithms
    - *Arithmetic density* of an algorithm: the number of arithmetic operations performed per byte read from or written to the system memory.
    - *Compute-bound algorithms* demonstrate high arithmetic density. Their performance is limited by compute speed.
    - *I/O-bound* or *memory-bound* algorithms reveal low arithmetic density. Their performance is constrained by memory bandwidth.

    > Contrary to common belief, modern imaging problems and their ML implementations tend to be **I/O-bound**.
    </textarea>
  </section>
</section>


<!-- Outline -->
<section>
  <h1>Outline</h1>
  <ol>
    <li style="color: lightgray">Introduction</li>

    <li>Programmable hardware overview
      <ul>
        <li>Central Processing Units (<strong>CPU</strong>)
          <ul>
            <li>x86 vs ARM</li>
            <li>Memory model</li>
            <li>Caching, SIMD, multithreading</li>
          </ul>
        </li>
        <li>Graphics Processing Units (<strong>GPU</strong>)
          <ul>
            <li>Graphics pipeline</li>
            <li>SIMT</li>
            <li>CUDA samples</li>
          </ul>
        </li>
        <li>Digital Signal Processors (<strong>DSP</strong>)
          <ul>
            <li>Quick overview</li>
          </ul>
        </li>
      </ul>
    </li>

    <li>Preparing ML models for production</li>

    <li>Conclusion</li>
  </ol>
</section>


<!-- Hardware overview  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
  </section>

  <section>
    <h2>Vanilla server/desktop system</h2>

    <img src="diagrams/PC.svg" class="r-stretch"/>

    <footer>
      <a href="https://en.wikipedia.org/wiki/File:Desktop_computer_bus_bandwidths.svg">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Mobile SoC (System on Chip)</h2>
    <h6>Example of Qualcomm&copy; Snapdragon&trade; 8+ Gen 1 (2022)</h6>

    <img src="diagrams/SoC.drawio.svg" class="r-stretch"/>

    <footer>
      <a href="https://www.forbes.com/sites/marcochiappetta/2021/11/30/snapdragon-8-gen-1-the-qualcomm-mobile-platform-that-will-power-next-gen-android-flagship-phones/">[source]</a>
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Mobile vs Desktop: Initial Observations
    - No unified memory in the desktop world.
      - Think ahead about CPU&leftrightarrow;GPU data transfer.
      - PCIe is the weak link (~10 times slower than the GPU VRAM bandwidth).
      - Modern x86 desktop and Apple Silicon CPUs also include integrated graphics (iGPU) without dedicated memory.
    - Digital Signal Processors (DSP) are very common in mobile SoCs.
      - *Edge computing*: processing data right on the device it comes from.
      - NPU is a marketing name/abstraction level on top of the DSPs.
    </textarea>
  </section>
</section>


<!-- Hardware overview: CPU  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
    <h3>Central Processing Units</h3>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## Basics
    - CPU is a general-purpose programmable device running programs built of *instructions.*
    - *Instruction set* is the language understood by a specific CPU architecture.
    - Common instruction types:
      - reading values from and writing them to main memory
      - arithmetic operations
      - control flow (branches and jumps).
    - A program is usually generated by *compiling* a higher-level source code.
    </textarea>
  </section>

  <section>
    <h2>Inside a Modern CPU</h2>

    <div class="flex">
      <div>
        <ul>
          <li>Inside each CPU core</li>
          <ul>
            <li>Instruction fetcher</li>
            <li>Instruction decoder</li>
            <li>Instruction queue (for out-of-order execution)</li>
            <li>Execution units (ALU, FPU, memory R/W, branch, etc)</li>
            <li>Branch prediction</li>
            <li>Register file (super fast memory, holds operands and results of arithmetic operations)</li>
          </ul>
          <li>Several instances of all the above: multicore architecture</li>
          <li>Cache (fast memory, hidden from the programmer)</li>
        </ul>
      </div>
      <div>
        <h5>Intel Skylake (2015)</h5>
        <img src="diagrams/skylake_block_diagram.svg">
      </div>
    </div>

    <footer>
      <a href="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(client)">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Common Architectures</h2>

    <table class="smaller">
      <thead>
        <tr>
          <td style="color: gray">Family</td>
          <td><strong>x86(_64)</strong></td>
          <td><strong>ARM</strong></td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="color: gray">Instruction set<br/>classification</td>
          <td>Complex Instruction Set Computer</br>(CISC)</td>
          <td>Reduced Instruction Set Computer</br>(RISC)</td>
        </tr>
        <tr>
          <td style="color: gray">Usage</td>
          <td>Mainly desktop/data center.<br/>Weak mobile market penetration.</td>
          <td>Mainly mobile.<br/>But: <a href="https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/">Apple Silicon</a>, <a href="https://www.nvidia.com/en-us/data-center/grace-cpu-superchip/">Nvidia Grace</a></td>
        </tr>
        <tr>
          <td style="color: gray">Key brand names</td>
          <td>Intel, AMD</td>
          <td>ARM, Apple, Samsung, Qualcomm, ...</td>
        </tr>
        <tr>
          <td style="color: gray">Word size</td>
          <td colspan="2" style="text-align: center">32 or 64 bits</td>
        </tr>
        <tr>
          <td style="color: gray">Vector extensions (SIMD)</td>
          <td>SSE (128 b), AVX (256 b), AVX-512</td>
          <td>NEON (128 b), SVE (variable)</td>
        </tr>
        <tr>
          <td style="color: gray">Simultaneous multithreading<br/>aka "Hyper-Threading"</td>
          <td>Yes:<br/>commonly 2 logical per 1 physical</td>
          <td>No, till recently: <a href="https://www.arm.com/products/silicon-ip-cpu/cortex-a/cortex-a65ae">Cortex A65</a></td>
        </tr>
      </tbody>
    </table>

    <p class="smaller">Other architectures exist. To keep an eye on: RISC-V.</p>
  </section>

  <section>
    <h2>Execution Speed</h2>

    <ul>
      <li><em>Clock cycle</em>: atomic value of elapsed time in a digital circuit<br>
        (0.2 ns at 5 GHz to 1 ns at 1 GHz).</li>
      <li><em>Decoder width:</em> number of instructions decoded simultaneously<br>
        (1 for very simple CPU, 4 for Intel/AMD, up to 8 in Apple Silicon).</li>
      <li><em>Latency:</em> cycles it takes before the result of an instruction is available for use in other instructions<br>
        (1 for OR/ADD, 3 or 4 for MUL, many for DIV (12 on Intel Skylake)).</li>
      <li><em>Throughput:</em> Average instructions per cycle<br>
        (0.05 to 6 for a single instruction type, increases for good instruction mix).</li>
      <li>Instruction latencies and throughputs are context-dependent but measurable.</li>
      <li>Multiply-accumulate (MAC) is typically a single 3-op instruction and faster than<br>
        multiplication followed by addition.</li>
      <li>Complex floating-point math instructions (e.g. <span class="code">sqrt</span>) may have content-dependent latency.</li>
    </ul>

    <footer>
      Unofficial but highly acknowledged sources for <a href="https://www.agner.org/optimize/instruction_tables.pdf">x86</a> and <a href="https://hardwarebug.org/2014/05/15/cortex-a7-instruction-cycle-timings/">ARM</a>.
    </footer>
  </section>

  <section>
    <h2>Single Instruction Multiple Data</h2>

    <ul>
      <li>Signal and image processing: apply the same instructions to many data samples.</li>
      <li>Problem: decoding the same instructions over and over again is a waste of resources.</li>
      <li>Solution: apply each instruction to several data samples.</li>
      <li>Vertical vector instruction<br>
        <img src="diagrams/vop.svg"></li>
      <li>Horizontal vector instruction<br>
        <img src="diagrams/hop.svg"></li>
      <li>Operate on <em>vectors</em> stored in a dedicated register file</li>
      <ul>
        <li>SSE, NEON: 128 bits = 2 &times; double or int64 / 4 &times; float or int32 / 8 &times; int16 / 16 &times; int8</li>
        <li>AVX: 256 bits = 4 &times; double or int64 / 8 &times; float or int32 / 16 &times; int16 / 32 &times; int8</li>
        <li>AVX-512: 512 bits = 8 &times; double or int64 / 16 &times; float or int32 / 32 &times; int16 / 64 &times; int8</li>
        <li>SVE: Register size is implementation dependent</li>
      </ul>
    </ul>
  </section>

  <section>
    <h2>Programming for SIMD</h2>

    <ul>
      <li>Control flow is the same for all samples (no if/else)!<br>
        &rarr; Compute both, then choose.</li>
      <li>Cannot use recursion.<br>
        &rarr; Use a different algorithm.</li>
      <li>Sample number must be a multiple of vector size.<br>
        &rarr; Round up in memory allocation, process a few useless samples.</li>
      <li>Vector elements must/should be contiguous in memory.<br>
        &rarr; Prefer structure of arrays over array of structures.</li>
      <li>Compilers are bad at vectorizing (and programmers are bad at writing vectorizable code).<br>
        &rarr; Use intrinsic functions to force both programmers and compilers.</li>
      <li>Along which dimension should we vectorize an image?</li>
      <ul>
        <li>Channels: RGB images, 4-element vectors: introduce useless 4th channel (<em>opacity</em>).<br>
          &rarr; Store images as RGBA|RGBA|&mldr;</li>
        <li>Rows: more flexible (works for any number of channels and any vector size).<br>
          &rarr; For RGB images, store each channel in its own location.</li>
        <li>Columns: bad idea (pixels are not stored contiguously).</li>
      </ul>
    </ul>

    <footer>
      See intrinsics references for <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">x86</a> and <a href="https://https://developer.arm.com/architectures/instruction-sets/intrinsics/">ARM</a>
    </footer>
  </section>

  <section>
    <h2>Multitasking, Multiprocessing, Multithreading</h2>

    <ul>
      <li>CPUs typically work on more than one task at a time:<br>
        Display slides, check mail in background, install updates&mldr;</li>
      <li><em>Preemptive multitasking:</em> OS switches task every few milliseconds (1 ms = 1 Mio cycles).</li>
      <li><em>Multiprocessing:</em> several CPUs (or one CPU with several <em>cores</em>) execute several tasks in parallel.</li>
      <li><em>Heterogeneous topology:</em> different cores (using the same instruction set) inside one CPU:<br>
        Performance and efficiency cores.</li>
      <li><em>Multithreading:</em> inside a single <em>process</em>, split execution in several <em>threads</em> which execute in parallel.<br>
        Example mail client: one thread for the UI, one for talking to the server, one for search indexing.<br>
        Because slow tasks run in the background, the UI remains responsive.</li>
      <li><em>Simultaneous multithreading</em> (SMT) aka <em>Hyperthreading:</em> one physical core pretends to be two cores,<br>
        fetches instructions from two different threads and mixes the instructions in its instruction queue.</li>
      <li>Multithreading allows each thread to work on a completely different task.<br>
        But we can also use it to split one big task (image processing) among several cores.</li>
    </ul>
  </section>

  <section>
    <h2>Programming for Multithreading</h2>

    <ul>
      <li>Multiple threads must not write to the same memory address at the same time (bug).</li>
      <li>Multiple threads should avoid writing to neighboring memory addresses at the same time (slow).</li>
      <li>Execution speed can vary between threads.<br>
        Interrupted by the OS or running on different types of cores.</li>
      <li>Synchronizing data between threads is hard (requires mutexes and atomic variables).<br>
        Better let different threads work on unrelated problems.</li>
      <li>Recipe: divide your image into <em>tiles</em> and process each tile in a separate thread.</li>
      <li>Neighborhood filters require overlap between tiles to produce the correct output.</li>
      <li>How many tiles? What is the best tile size and shape?</li>
      <ul>
        <li>To keep all cores busy, need much more tiles than cores<br>
          (5-10 times more to avoid waiting for the last tile).</li>
        <li>Creating a thread is not free, only create threads that have a significant amount of work to do<br>
          (one thread per pixel is a very bad idea).</li>
        <li>Square tiles are best to minimize overlap (example: 256 &times; 256).</li>
        <li>Horizontal bands are best to have consecutive memory access (example: 4000 &times; 16).</li>
      </ul>
    </ul>

    <footer>
      Use <a href="https://en.cppreference.com/w/cpp/thread/thread">std::thread</a> or try <a href="https://www.openmp.org">OpenMP</a>
    </footer>
  </section>

  <section>
    <h2>Memory Hierarchy</h2>

    <div class="flex">
      <ul class="flex-col-75">
        <li><em>Size:</em> number of bytes that can be stored.</li>
        <li><em>Bandwidth:</em> number of bytes that can be read per cycle.</li>
        <li><em>Latency:</em> number of cycles it takes to read or write one value.</li>
        <li>Memory cannot be big and fast at the same time.</li>
        <li>Gap between huge (but slow) main memory and superfast (but tiny) register file is too big.</li>
        <li><em>Cache:</em> hidden (from the programmer) intermediate memories with different size/speed trade-off.</li>
        <li>Based on the assumption of locality:
          <ul>
            <li>Temporal locality: program frequently reads and writes the same address.</li>
            <li>Spatial locality: program frequently reads and writes consecutive memory addresses.</li>
          </ul>
        </li>
        <li><em>Cache miss:</em> occurring when the requested memory address is not found in a given cache.
        <li><em>Stalled thread:</em> thread waiting for data from memory.
        <li>SMT: cache is shared between two threads.</li>
      </ul>
      <img class="flex-col-25" src="diagrams/Multi-core_cache_pyramid.svg">
    </div>
  </section>

  <section>
    <h2>Memory Access Speed</h2>

    <div class="flex">
      <div>
        <h5>Intel Core i7-9xxx</h5>
        <ul>
          <li>L1 cache: 64 Kbytes per physical core, <strong>4&nbsp;cycles</strong>
            <ul>
              <li>32 Kbytes instruction cache + 32 Kbytes data cache</li>
            </ul>
          </li>
          <li>L2 cache: 256 Kbytes per physical core, <strong>11&nbsp;cycles</strong></li>
          <li>L3 cache: 8 Mbytes shared across cores, <strong>39&nbsp;cycles</strong></li>
          <li>RAM: <strong>107&nbsp;cycles</strong>.</li>
        </ul>
      </div>
      <div>
        <h5>A synthetic example</h5>
        <img src="images/memory_model.png" style="margin: 0em">
      </div>
    </div>
    <footer>
      <a href="https://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf">[source]</a>
      <a href="https://cs.brown.edu/courses/csci1310/2020/assign/labs/lab4.html">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Programming for Cache</h2>

    <ul>
      <li>Spatial locality: easy.
        <ul>
          <li>Store pixels consecutively in memory.</li>
          <li>Data is copied to cache by chunks of 64 bytes <em>(cache line)</em><br>
            &rarr; Wait for first pixel, get some more for free.</li>
          <li>CPU detects consecutive access patterns and applies <em>prefetching</em>.<br>
            &rarr; Wait for first pixel, get all others for free.</li>
        </ul>
      </li>
      <li>Temporal locality: hard.
        <ul>
          <li>Typical image processing operation (e.g., convolution) reads from one or more input tiles, applies some computation, and writes to an output tile. Typical algorithms consist of many operations.</li>
          <li>Tile of 256 &times; 256 pixels, RGB, float (4 bytes per sample): 768 kB &rarr; does not fit in L2 cache.</li>
          <li>Data must be fetched from L3 cache or even RAM every time.<br>
            Shared between all cores &rarr; bottleneck, reduces the benefit ot multithreading.</li>
          <li>Use smaller tiles? Better for cache but less efficient (overlap).</li>
          <li>Solution: row-based image processing.</li>
          <ul>
            <li>Execute algorithm line by line, apply each operation as early as you can.</li>
            <li>Very efficient, but code is hard to write.</li>
          </ul>
        </ul>
      </li>
    </ul>

    <footer>
      <q>It takes so long to get there [to main memory], your program will not run at any reasonable speed if you go to main memory.</q>
      <a href="https://youtu.be/WDIkqP4JbkE">[Scott Meyers]</a>
    </footer>
  </section>
</section>


<!-- Hardware overview: GPU  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
    <h3>Graphics Processing Units</h3>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## GPU: Brief History
     - In early times (~2000 AD), GPUs were <em>rasterization engines</em>.
       - They sampled images pixel by pixel from a vector description of renderable content, such as a set of 3D vertices, their attributes (e.g. texture coordinates), light source positions, and textures.
       - Suitable for real-time visualization and... gaming.
       - They featured a *fixed graphics pipeline* (no programmability).
     - The introduction of *pixel shaders* around 2001 marked a shift.
       - Pixel shaders were small programs executed on a per-pixel basis to compute the resulting color value.
       - Beginning of per-pixel thread granularity in real-time, hardly achievable on CPUs.
       - Nvidia GeForce 3 (NV20) and AMD Radeon R100
     - *Microsoft DirectX* and *OpenGL* standards have driven the GPU evolution towards programmability.
       - *GPGPU*: General-Purpose computing on Graphics Processing Units
    </textarea>
  </section>

  <section>
    <h2>Simplified Graphic Pipeline</h2>
    <img class="r-stretch" src="images/graphic_pipeline.jpg"/>
    <ul>
      <li><em>Vertex shader</em> transforms 3D positions of vertices into 2D on-screen coordinates.
        <ul>
          <li>Uses camera position, orientation and intrinsic parameters (e.g. field of view).</li>
          <li>May also compute <em>lighting parameters</em> (e.g., reflected light intensity and color) based on the light source positions and per-vertex normals.</li>
        </ul>
      </li>
      <li><em>Rasterizer</em> samples <em>fragments</em> out of <em>primitives</em> (e.g., triangles) in the 2D coordinate space.
        <ul>
          <li>It interpolates issued vertex attributes, e.g. <em>texture coordinates</em> and lighting parameters.</li>
        </ul>
      </li>
      <li><em>Fragment shader</em> uses the per-fragment interpolated attributes to get pixel colors.
        <ul>
          <li>Usually, it samples a texture at given texture coordinates to get the color value.</li>
          <li>May use lighting parameters to modulate the resulting color.</li>
        </ul>
      </li>
    </ul>
    <footer style="margin: 0">
      <a href="https://stanford.edu/class/ee267/lectures/lecture2.pdf">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Programmable Shading Example: Lighting</h2>
    <img class="r-stretch" src="images/unreal_shading_example.jpg"/>
    <footer style="margin: 0">
      <a href="https://www.nvidia.com/content/gtc-2010/pdfs/2275_gtc2010.pdf">[source]</a>
    </footer>
  </section>

  <section>
    <h2>GPU Compute Evolution: Back Then</h2>
    <img src="images/nvidia_gflops_evolution.png" class="r-stretch"/>
    <p class="smaller">*AlexNet was trained on two GTX580 <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">[Krizhevsky et al.]</a></p>

    <footer style="margin: 0">
      <a href="https://www.researchgate.net/publication/272434636_Accelerating_the_Detection_of_Spectral_Bands_by_ANN-ED_on_a_GPU#pf2">[source]</a>
    </footer>
  </section>

  <section>
    <h2>GPU Compute Evolution: These Days</h2>
    <ul>
      <li><em>Quadro RTX 6000</em> (Turing architecture, 2018):
        <ul class="no-bullets">
          <li><strong>16.3 TFLOPs</strong> single precision FP</li>
          <li><strong>32.6 TFLOPs</strong> half precision FP</li>
          <li><strong>130.5 TFLOPs</strong> half precision FP with TensorCores <span style="color: red">(new)</span></li>
        </ul>
      </li>
      <li><em>Tesla A100</em> (Ampere architecture, 2020):
        <ul class="no-bullets">
          <li><strong>19.5 TFLOPs</strong> single precision FP</li>
          <li><strong>156 TFLOPs</strong> single precision FP with TensorCores <span style="color: red">(new)</span></li>
          <li><strong>312 TFLOPs</strong> half precision FP with TensorCores</li>
        </ul>
      </li>
      <li><em>Tesla H100</em> (Hopper architecture, 2022):
        <ul class="no-bullets">
          <li><strong>48 TFLOPs</strong> single precision FP</li>
          <li><strong>400 TFLOPs</strong> single precision FP with TensorCores</li>
          <li><strong>800 TFLOPs</strong> half precision FP with TensorCores</li>
          <li><strong>1600 TFLOPs</strong> 8-bit FP with TensorCores <span style="color: red">(new)</span></li>
        </ul>
      </li>
    </ul>
    <footer style="margin: 0">
      <a href="https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/">[Turing]</a>
      <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">[Ampere]</a>
      <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">[Hopper]</a>
      <a href="https://arxiv.org/pdf/2209.05433.pdf">[FP8 whitepaper]</a>
    </footer>
  </section>

  <section>
    <h2>Programming and Memory Model</h2>
    <ul>
      <li>GPUs achieve high throughput by a massive parallelization, which requires designing the software in a particular way.</li>
      <li>Let us took a look at memory first.</li>
    </ul>
  </section>

  <section>
    <h2>A GPU From Inside (Nvidia Ampere)</h2>
    <img src="images/nvidia_ampere.png" class="r-stretch"/>
    <br/>
    <small>Things to point out: PCIe interface, HBM2 (<em>global memory</em>), SMs with shared L2 cache, NVLink and MIG</small>
  </section>

  <section>
    <h2>Closer Look at Ampere Streaming Multiprocessor</h2>
    <img src="images/nvidia_ampere_sm.png" class="r-stretch"/>
    <br/>
    <small>Things to point out: instruction caches, L1 data cache, register file, compute units, warp schedulers.</small>
  </section>

  <section data-markdown>
    <textarea data-template>
     ## Memory Model (Ampere Numbers)
      1. **Register file**: 64 Kbytes &times; 4 per SM
      2. **L1 data cache**: 192 Kbytes per SM
         - L1 can be used as *shared memory* directly managed by the program.
      3. **L2 data cache**: 40 Mbytes shared across SMs
      4. **Global memory**: ~40~/80 Gbytes for A100
      5. Anything beyond goes through PCIe to/from the host.
    </textarea>
  </section>

  <section>
    <h2>Common GPU Programming Frameworks</h2>
    <ul>
      <li>
        <strong>CUDA</strong> (Compute Unified Device Architecture)
        <ul>
          <li>Parallel computing SDK for Nvidia GPUs</li>
          <li>C/C++-like language and a compilation toolchain</li>
          <li>Bundled with precompiled compute libraries (cuBLAS, cuDNN, cuFFT, etc.)
            <ul>
              <li>Mainly closed source except for CUTLASS</li>
            </ul>
          </li>
          <li>Primary compute backend for modern ML: PyTorch and TensorFlow rely heavily on cuDNN/cuBLAS</li>
        </ul>
      </li>
      <li>
        <strong>ROCm</strong> (Radeon Open Compute)
        <ul>
          <li>Parallel computing SDK for AMD GPUs</li>
          <li>Offers multiple programming models: OpenCL, HIP and OpenMP</li>
          <li>Open-source</li>
          <li>Officially supports a subset of the AMD GPU product line</li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>Common GPU Programming Frameworks</h2>
    <ul>
      <li>
        <strong>OpenCL</strong> (Apple &rightarrow; <em>Khronos Group</em>)
        <ul>
          <li>Parallel computing software stack to build programs from a C/C++-like language</li>
          <li>Usable within a wide hardware spectrum, including GPUs, CPUs, FPGAs and other kinds of accelerators</li>
        </ul>
      </li>
      <li>
        <strong>GLSL</strong> (OpenGL Shading Language, <em>Khronos Group</em>)
        <ul>
          <li>C-like programming language supported by a vast spectrum of graphics hardware</li>
          <li>Originally computer graphics-specific, now enables general purpose parallel programming (<em>compute shaders</em>)
            <ul>
              <li>Compute shaders are not available everywhere</li>
            </ul>
          </li>
          <li>Requires a host framework (OpenGL, Vulkan, WebGL)</li>
          <li>Typically compiled at run time by the GPU driver into GPU-specific machine code</li>
        </ul>
      </li>
    </ul>
  </section>

  <section>
    <h2>SIMT: Single Instruction Multiple Threads</h2>
    <ul>
      <li style="list-style: none;">SIMT is a parallel program execution model GPUs are built upon.</li>
      <li style="list-style: none;">
        <div class="highlight">
          It provides a hardware abstraction convenient for both programmers and hardware manufacturers to decouple the program source code and device capabilities in a scalable way.
        </div>
      </li>
      <li>The programs (<em>shaders</em>, <em>kernels</em>) are written in a way to handle a single data entry all over the lifecycle.
        <ul>
          <li>Typical granularity: 1 thread per pixel.</li>
        </ul>
      </li>
      <li>The hardware and its driver take care of threading.
        <ul>
          <li>To specify the amount of work, threads are grouped into <em>blocks</em> (<em>workgroups</em>), which form a <em>grid</em>.</li>
          <li>Threads in the same block can exchange information efficiently: they have access to the same low-latency shared memory space.
            <ul>
              <li>In practice, the HW runs them close to each other, in time and memory.</li>
            </ul>
          </li>
          <li>The grid configuration is set when launching the program.</li>
        </ul>
        </li>
      <li>At runtime, the same instruction is issued in multiple threads, <em>possibly</em> running in parallel.</li>
    </ul>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## CUDA Kernel Example
     - The program that follows evaluates expression <code>a*x<sub>i</sub>+y<sub>i</sub></code>
       - <code>x</code> and <code>y</code> are large arrays of the same size.
       - <code>a</code> is a constant.
     - It illustrates a typical GPU compute workflow.
       - Preparing the data in CPU main memory.
       - Uploading the data to GPU global memory.
       - Launching the computing kernel.
       - Fetching the computation result back for further processing.
     - Compiling and executing (linux, requires CUDA toolkit installed):
    ```
    nvcc ./saxpy.cu -o saxpy
    ./saxpy
    ```
    </textarea>
  </section>

  <section>
    <h2>CUDA Kernel Example</h2>
    <pre class="stretch"><code data-trim data-noescape style="font-size: 0.9em; line-height: 1.2;" class="cpp">
#include &lt;cstdio&gt;
#include &lt;vector&gt;

__global__ void saxpy(int n, float a, float *x, float *y) {                   <em class="fragment" data-fragment-index="6">// &lt;-- The kernel function executed on GPU</em>
  int i = blockIdx.x * blockDim.x + threadIdx.x;                              <em class="fragment" data-fragment-index="7">// Find out current thread position</em>
  if (i &lt; n) y[i] = a * x[i] + y[i];                                          <em class="fragment" data-fragment-index="8">// Compute the result</em>
}

int main(void) {
  const int N = 1 &lt;&lt; 20;
  std::vector&lt;float&gt; x(N), y(N);                                              <em class="fragment" data-fragment-index="1">// Declare storage for test data (CPU memory)</em>

  for (int i = 0; i &lt; N; i++) {                                               <em class="fragment" data-fragment-index="2">// Fill test data buffers with some numbers (in CPU memory)</em>
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  float *d_x, *d_y;                                                           <em class="fragment" data-fragment-index="3">// Declare pointers in GPU memory</em>
  cudaMalloc(&d_x, N * sizeof(float));                                        <em class="fragment" data-fragment-index="3">// Allocate GPU buffer to store x</em>
  cudaMalloc(&d_y, N * sizeof(float));                                        <em class="fragment" data-fragment-index="3">// Allocate GPU buffer to store y</em>

  cudaMemcpy(d_x, x.data(), N * sizeof(float), cudaMemcpyHostToDevice);       <em class="fragment" data-fragment-index="4">// Copy x from CPU memory to GPU</em>
  cudaMemcpy(d_y, y.data(), N * sizeof(float), cudaMemcpyHostToDevice);       <em class="fragment" data-fragment-index="4">// Copy y from CPU memory to GPU</em>

  saxpy&lt;&lt;&lt;(N+255)/256, 256&gt;&gt;&gt;(N, 2.0f, d_x, d_y);                             <em class="fragment" data-fragment-index="5">// Launch the kernel, 1 thread per element, 256 thread per block</em>

  cudaMemcpy(y.data(), d_y, N * sizeof(float), cudaMemcpyDeviceToHost);       <em class="fragment" data-fragment-index="9">// Copy the result back from GPU</em>

  float maxError = 0.0f;                                                      <em class="fragment" data-fragment-index="10">// Check the result</em>
  for (int i = 0; i &lt; N; i++)
    maxError = max(maxError, abs(y[i] - 4.0f));
  printf("Max error: %f\n", maxError);

  cudaFree(d_x);                                                              <em class="fragment" data-fragment-index="11">// Free the GPU buffer</em>
  cudaFree(d_y);                                                              <em class="fragment" data-fragment-index="11">// Free the GPU buffer</em>
}
    </code></pre>
    <footer style="margin: 0">
      <a href="https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Occupancy</h2>
    <ul class="no-bullets">
      <li>Performance is conditioned by the number of threads effectively running in parallel. It depends on:
        <ul>
        <li><em>Register pressure</em>: amount of register memory required per thread vs hardware capacity
          <ul>
            <li>Modern CUDA: 255 regular registers per thread max</li>
            <li><em>Register spilling</em>: if the register file size is not sufficient to store all the registers, the data may be stored in global memory.</li>
            <li>Register spilling is not permitted in all SIMT implementations and often causes a dramatic slowdown.</li>
          </ul>
        </li>
        <li>Amount of shared memory per thread block vs the hardware capacity
          <ul>
            <li>CUDA: 48 to 163 Kbytes depending on CUDA Compute Capability</li>
            <li>OpenGL ES 3.1 Compute Shader: 16 Kbytes as a minimum required by the standard</li>
          </ul>
        </li>
        <li>SM capabilities (e.g. max number of warps per SM)
          <ul>
            <li><em>Warp</em> (CUDA) is a group of 32 consecutive threads.</li>
            <li>CUDA: 1024 threads per thread block max</li>
            <li>OpenGL ES 3.1 Compute Shader: 128 threads per <em>workgroup</em> as a minimum required by the standard</li>
          </ul>
        </li>
        <li>Total number of SMs</li>
        </ul>
      </li>
    </ul>
    <footer>
      <a href="https://registry.khronos.org/OpenGL/specs/es/3.1/es_spec_3.1.pdf">[OpenGL ES 3.1 Spec]</a>
      <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">[CUDA CC]</a>
    </footer>
  </section>

  <section>
    <h2>Divergent threads</h2>
    <pre><code data-trim data-noescape class="cpp">
  // ...
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (mat[i] > 0) {
    // Branch A
  }
  else {
    // Branch B
  }
  // ...
    </code></pre>
    <ul>
      <li>If condition <code>mat[i] > 0</code> or its opposite holds for the entire warp, mostly no impact on performance.</li>
      <li>If condition <code>mat[i] > 0</code> holds for some threads but not for others in the same warp, both branches A and B will be executed using an "active threads mask", inducing a performance penalty.</li>
    </ul>
    &rightarrow; Common recommendation: align data-dependent execution paths with warp boundaries
    <footer>
      <a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">[Volta whitepaper], p.26</a>
    </footer>
  </section>

  <section>
    <h2>Coalesced vs Non-coalesced Access</h2>
    <ul class="no-bullets">
      <li>Consider a CUDA kernel going through a big square matrix of size <code>N</code>.</li>
      <li></li>
    </ul>

    <div class="flex">
      <div class="flex-col">
        <h5>Version 1:</h5>
        <pre><code data-trim data-noescape class="cpp">
__global__ void matrix_traversal(float *mat, int N) {
  int col = blockIdx.x * 32 + threadIdx.x;
  int row = blockIdx.y * 32 + threadIdx.y;
  float element = mat[col * N + row];
  // ...
}
        </code></pre>
      </div>
      <div class="flex-col">
        <h5>Version 2:</h5>
        <pre><code data-trim data-noescape class="cpp">
__global__ void matrix_traversal(float *mat, int N) {
  int col = blockIdx.x * 32 + threadIdx.x;
  int row = blockIdx.y * 32 + threadIdx.y;
  float element = mat[row * N + col];
  // ...
}
        </code></pre>
      </div>
    </div>

    <h4>Kernel launch code:</h4>
    <pre><code data-trim data-noescape class="cpp">
      const dim3 threads(32, 32);       // processing the matrix by tiles of 32*32 elements
      const dim3 blocks(N/32, N/32);    // assuming N is a multiple of 32
      matrix_traversal&lt;&lt;&lt; blocks, threads &gt;&gt;&gt;(matrixDataDevicePtr, N);
    </code></pre>

    <p>Question: which implementation is more efficient (and, of course, why)?</p>
  </section>

  <section>
    <h2>Coalesced vs Non-coalesced Access</h2>
    <div class="flex">
      <div class="flex-col">
        <h5>Version 1:</h5>
        <pre><code data-trim data-noescape class="cpp">
__global__ void matrix_traversal(float *mat, int N) {
  int col = blockIdx.x * 32 + threadIdx.x;
  int row = blockIdx.y * 32 + threadIdx.y;
  float element = mat[col * N + row];       <strong>//&lt;---- LOAD</strong>
  // ...
}
        </code></pre>
      </div>
      <div class="flex-col">
        <h5>Version 2:</h5>
        <pre><code data-trim data-noescape class="cpp">
__global__ void matrix_traversal(float *mat, int N) {
  int col = blockIdx.x * 32 + threadIdx.x;
  int row = blockIdx.y * 32 + threadIdx.y;
  float element = mat[row * N + col];       <strong>//&lt;---- LOAD</strong>
  // ...
}
        </code></pre>
      </div>
    </div>

    <ul>
      <li>Nvidia GPU cache line size: 128 bytes (a hardware constant)</li>
      <li>Global memory access is serialized.</li>
    </ul>
    <p></p>

    <div class="flex">
      <div style="flex: 1 1 100%; text-align: left">
        <ul>
          <li class="fragment" data-fragment-index="1">Relative addresses per thread:<br/> <code>[0, 4N, 8N, ..., 32N]</code></li>
          <li class="fragment" data-fragment-index="2">Large <code>N</code>: every address falls into a different cache line</li>
          <li class="fragment" data-fragment-index="3"><strong>32</strong> cache lines to load per warp</li>
          <li class="fragment" data-fragment-index="4">This is slow.</li>
        </ul>
      </div>
      <div style="flex: 1 1 100%; text-align: left">
        <ul>
          <li class="fragment" data-fragment-index="1">Relative addresses per thread:<br/> <code>[0, 4, 8, ..., 124]</code></li>
          <li class="fragment" data-fragment-index="2">All the addresses are in the same cache line</li>
          <li class="fragment" data-fragment-index="3"><strong>1</strong> cache line to load per warp</li>
          <li class="fragment" data-fragment-index="4">This is fast.</li>
        </ul>
      </div>
    </div>
  </section>

  <section>
    <h2>Non-coalesced Access: What to Do?</h2>
    <ul class="no-bullets">
      <li>It is not always possible to change the way the memory is accessed.</li>
      <li>A common shared memory transposition trick:
        <ul>
          <li>A tile is read in a coalesced manner and put into a fast shared memory buffer.</li>
          <li>The transposition is done when reading from it.</li>
          <li>Every thread receives the same value as before.</li>
        </ul>
      </li>
    </ul>
    <p></p>
    <h5>Version 1 upgrade:</h5>
    <pre><code data-trim data-noescape class="cpp">
__global__ void matrix_traversal(float *mat, int N) {
  <strong>__shared__</strong> float buffer[32 * 32];
  int startCol = blockIdx.x * 32;
  int startRow = blockIdx.y * 32;

  int i = (startCol + threadIdx.y) * N + (startRow + threadIdx.x);
  buffer[threadIdx.y * 32 + threadIdx.x] = mat[i];    // now coalesced
  __syncthreads();                                    // waiting for all threads in the same block to pass through
  float element = buffer[threadIdx.x * 32 + threadIdx.y];
  // ...
}
    </code></pre>
  </section>

  <section>
    <h3>... and there is a lot more.</h3>
    <img src="images/here_be_ninjas.png" class="r-frame">
    <footer>
      <a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">[Stephen Jones, 2017]</a>
    </footer>
  </section>

  <section data-markdown>
    <textarea data-template>
    ## GPU: Takeaways
     - GPUs are widely available massively parallel programmable machines.
     - GPU execution model is SIMT.
       - Threads &rightarrow; Warps &rightarrow; Blocks &rightarrow; Grid (CUDA language)
     - The amount of parallelism is characterized by occupancy.
     - Memory hierarchy: same principles hold (*"fast is small"*).
       - Coalesced access is critical for performance.
       - L1 cache can be used as program-manageable shared memory.
       - Mind data-dependent divergence.
    </textarea>
  </section>
</section>


<!-- Hardware overview: DSP  -->
<section>
  <section>
    <h2>Programmable Hardware Overview</h2>
    <h3>Digital Signal Processors</h3>
  </section>

  <section>
    <h2>DSP: Overview (Hexagon V67 Example)</h2>
    <ul>
      <li>Implements a specific SIMD instruction set
        <ul>
          <li><em>VLIW (Very Long Instruction Word)</em>: grouping independent instructions in packets for parallel execution at compile time.</li>
        </ul>
      </li>
      <li>May run multiple hardware threads</li>
      <li>Handles integer, fixed- and floating-point compute, as well as special data types (e.g. complex numbers)</li>
      <li>Instruction set includes "special-purpose" application-specific instructions
        <ul>
          <li>Video coding</li>
          <li>Software-defined radio</li>
          <li>Checksum computation</li>
          <li>...</li>
        </ul>
      </li>
      <li>Building programs requires a specific SDK</li>
    </ul>
    <footer>
      <a href="https://developer.qualcomm.com/qfile/67417/80-n2040-45_b_qualcomm_hexagon_v67_programmer_reference_manual.pdf">[Hexagon V67 reference manual]</a>
    </footer>
  </section>

  <section>
    <h2>Benchmarks?</h2>
    <ul class="no-bullets">
      <li>Public vanilla FLOPs benchmarks are hard to find.</li>
      <li>From a ML perspective: <a href="https://ai-benchmark.com/ranking_detailed.html">AI benchmark</a> or <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/benchmark_mobilenet_ssd.html">MobileNet SSD  SNPE sample</a>
        <ul>
          <li>DSP is much beefier compared to GPU and CPU.</li>
          <li>It is essentially a fixed-point machine though: it requires <em>quantized models</em>.</li>
          <li>Usable by means of Qualcomm Snapdragon Neural Processing Engine (SNPE) SDK and Android NN (e.g., TensorFlow Lite)</li>
        </ul>
      </li>
    </ul>
  </section>
</section>


<!-- Preparing ML models for production  -->
<section>
  <section>
    <h2>Preparing ML models for production</h2>
  </section>

  <section data-markdown>
    <textarea data-template>
      ## Production constraints
       - Common situation: a model is too big for a given production environment. <!-- .element: class="no-bullets"  -->
         - Inference on the target hardware/environment takes too much time or does not run at all.
       - Solution: get a simpler model... <!-- .element: class="no-bullets"  -->
         - Redesign the architecture by hand and train from scratch
           - Use *separable (depthwise+pointwise)* convolutions [(Xception, [Chollet, 2017])](https://arxiv.org/abs/1610.02357) or *group convolutions* [(ResNeXt, [Xie et al. 2016])](https://arxiv.org/pdf/1611.05431.pdf)
         - Train a smaller model by *knowledge distillation* [[Hinton et al., 2015]](https://arxiv.org/abs/1503.02531)
         - Use a pruning algorithm [&rightarrow; Awesome Pruning](https://github.com/he-y/Awesome-Pruning)
         - Use a constrained neural architecture search (NAS) algorithm [&rightarrow; Awesome AutoDL](https://github.com/D-X-Y/Awesome-AutoDL)
    </textarea>
  </section>

  <section>
    <h2>Production constraints</h2>
    <ul>
      <li class="no-bullets">Another common situation: the target hardware is fixed-point.</li>
      <li class="arrow-bullets">It requires quantized NN weights and applies quantization to activations over a given number of bits per entry,
        <ul>
          <li>up to a linear transformation,</li>
          <li>on per-layer or per-channel basis.</li>
        </ul>
      </li>
      <li>Example: Hexagon DSP, Snapdragon 8 Gen 1 (2021), the best you can get from SNPE:
        <ul>
          <li>16 bits per activation value</li>
          <li>8 bits per weight value
            <ul class="cross-bullets">
              <li>Insufficient for raw image restoration algorithms</li>
            </ul>
          </li>
          <li>32 bits per bias value</li>
        </ul>
      </li>
      <li class="no-bullets">Fixed-point model is a strict constraint in this example.</li>
      <li class="no-bullets">In general, fixed-point models are faster (at inference time), have reduced inference memory footprint and take less storage.</li>
    </ul>
    <footer>
      <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/tools.html#tools_snpe-dlc-quantize">[SNPE SDK docs]</a>
    </footer>
  </section>

  <section>
    <h2>Reminder: Floating Point vs Fixed Point</h2>
    <div class="flex">
      <ul class="flex-col">
        <li class="no-bullets"><strong>Floating point:</strong>
          <ul>
            <li>Dedicated compute units</li>
            <li>Infinite range (+Inf and -Inf are valid numbers)</li>
            <li>Higher accuracy around zero</li>
            <li>Divison by zero is allowed</li>
            <li>NaN</li>
          </ul>
        </li>
        <li class="no-bullets"><strong>Fixed point:</strong>
          <ul>
            <li>Integer compute, bit shift operations</li>
            <li>Finite range</li>
            <li>Uniform accuracy</li>
            <li>Division by zero leads to an exception</li>
          </ul>
        </li>
      </ul>
      <img src="images/floating_point.jpg" class="flex-col"/>
    </div>
    <footer>
      <a href="https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/">[source]</a>
    </footer>
  </section>

  <section>
    <h2>Tensor quantization</h2>
    <p>
      <em>Quantization</em> of a tensor: computing its approximate integer-valued representation in a given range.
    </p>
    <p>
      Example: FP32 tensor &rightarrow; INT8 tensor, <code>min</code> and <code>max</code> values
    </p>
    <img src="images/tensor_quantization.png" class="r-stretch"/>
    <ul class="smaller">
      <li>Linear operations (namely, convolution) can be computed using integer arithmetics. The result is then stretched according to min and max values of input and kernel.</li>
      <li>Non-linear activation functions require specific implementations (usually straightforward).</li>
    </ul>
    <footer>
      <a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/">[Quantization in TensorRT, Nvidia]</a>
    </footer>
  </section>

  <section>
    <h2>Model quantization</h2>
    <ul>
      <li class="no-bullets"><em>Quantization</em>: producing fixed-point model parameters while keeping inference accuracy.</li>
      <li><strong>Post-training quantization</strong>: adjusting parameters of an already trained model.
        <ul>
          <li>Parameters: quantizing as tensors.</li>
          <li>Activations: <em>calibrating</em> min and max by running inference on a representative dataset and collecting statistics.</li>
        </ul>
      </li>

      <li class="check-bullets">No need to retrain the model</li>
      <li class="cross-bullets">Data dependency: requires a representative dataset to calibrate the quantization parameters</li>
      <li class="cross-bullets">Limited: may lead to poor accuracy</li>
      <li class="no-bullets">Advanced approaches exist, e.g., <em>cross-layer equalization</em> for models using ReLU <a href="https://arxiv.org/pdf/1906.04721.pdf">[Nagel et al., 2018]</a></li>
    </ul>
  </section>

  <section>
    <h2>Model quantization</h2>
    <ul>
      <li class="no-bullets"><em>Quantization</em>: producing fixed-point model parameters while keeping inference accuracy.</li>
      <li><strong>Quantization-aware training</strong> (QAT): introducing quantization during the training.</li>
        <ul>
          <li>Parameters: using quantized representation during the forward pass. The optimizer updates the original (floating-point) version after the backward pass.</li>
          <li>Activations:
            <ul>
              <li>Forward pass: applying quantization as an additional activation function.</li>
              <li>Backward pass: only zeroing gradients for entries falling out of the valid range.
                <ul>
                  <li>Quantization acting as an activation function has zero gradient everywhere, so cannot backpropagate through.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <footer>
      <a href="https://arxiv.org/pdf/1806.08342.pdf">[Quantization whitepaper by Google]</a>
    </footer>
  </section>
</section>

<section data-markdown>
  <textarea data-template>
    ## Conclusion
     - Know your hardware to get the most of it.
       - Image, signal processing and modern ML tend to be I/O-bound.
       - Mind memory layout, locality and caching.
     - GPUs are ubiquitous and suitable for imaging tasks.
       - Occupancy is the key performance factor.
       - Keep memory access coalesced and avoid divergent threads in the same warp.
     - Go fixed-point whenever you can.
       - ML: getting to a quantized model with good accuracy requires additional effort.
       - Mind low-precision floating point formats as well.
  </textarea>
</section>

</div>  <!-- class="reveal" -->
</div>  <!-- class="slides" -->

<!-- END OF CONTENT -->

<!-- Loading plugins here, zoom fails otherwise -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/markdown/markdown.min.js" integrity="sha512-ik0i3CQNgrNfRPUyZGlVqKGpxWR//UV0Y+4y6FHeHvaX/T/TM2nLyjCDQr6Nl/VZrNF8zjkiQ/YL0bsT9GsbVQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/zoom/zoom.min.js" integrity="sha512-Gras1ky8LoFJWwMTxBWyN2wfHfnJXQlyhHFH3M+m/jHe297DZsrQxg9P6Kxka6waxl4NeeQzietoFlCxL7x10g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/highlight.min.js" integrity="sha512-PrMZUaiqYqRQyi4D71cIxqg/nyiclXEBLlR6A76tQP81iq6vrz9DBa+vLkn2efw3ECfYbKH1G+uLLpq6ZZ4dLA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script>
  (function(){
    Reveal.initialize({
      width: 1900,
      height: 1080,
      hash: true,
      plugins: [ RevealMarkdown, RevealZoom, RevealHighlight ]
    });
  })()
</script>

</body>
</html>
